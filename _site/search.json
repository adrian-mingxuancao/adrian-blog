[
  {
    "objectID": "blog-posts/alphagenome-evolution-genomic-modeling.html",
    "href": "blog-posts/alphagenome-evolution-genomic-modeling.html",
    "title": "AlphaGenome Evolution: Advancing Regulatory Variant Prediction and Genomic Modeling",
    "section": "",
    "text": "AlphaGenome is a deep learning model that processes 1 megabase (Mb) of DNA sequence to predict a broad range of functional genomic outputs at high resolution. Developed by Google DeepMind, AlphaGenome unifies multimodal genomic prediction, long-range sequence context, and single base-pair resolution into one framework. It generates 5,930 human (or 1,128 mouse) genome tracks covering 11 data modalities – including gene expression (RNA-seq, CAGE, PRO-cap), splicing (splice sites, splice site usage, splice junctions), chromatin accessibility (DNase, ATAC-seq), histone marks, transcription factor binding, and 3D chromatin contacts. Trained jointly on human and mouse genomes, AlphaGenome achieves or exceeds state-of-the-art performance on the vast majority of benchmarks, matching or outperforming the best available models on 24 out of 26 variant effect prediction tasks. In doing so, it addresses longstanding trade-offs in genome modeling by capturing both distal regulatory context and nucleotide-level detail, enabling more accurate predictions of how genetic variants influence gene regulation. This report provides a detailed overview of AlphaGenome’s methodology – including its architecture, training strategy, improvements over the earlier Enformer model, variant effect prediction performance across multiple modalities, and speculations on integrating AlphaGenome with genomic language models."
  },
  {
    "objectID": "blog-posts/alphagenome-evolution-genomic-modeling.html#introduction",
    "href": "blog-posts/alphagenome-evolution-genomic-modeling.html#introduction",
    "title": "AlphaGenome Evolution: Advancing Regulatory Variant Prediction and Genomic Modeling",
    "section": "",
    "text": "AlphaGenome is a deep learning model that processes 1 megabase (Mb) of DNA sequence to predict a broad range of functional genomic outputs at high resolution. Developed by Google DeepMind, AlphaGenome unifies multimodal genomic prediction, long-range sequence context, and single base-pair resolution into one framework. It generates 5,930 human (or 1,128 mouse) genome tracks covering 11 data modalities – including gene expression (RNA-seq, CAGE, PRO-cap), splicing (splice sites, splice site usage, splice junctions), chromatin accessibility (DNase, ATAC-seq), histone marks, transcription factor binding, and 3D chromatin contacts. Trained jointly on human and mouse genomes, AlphaGenome achieves or exceeds state-of-the-art performance on the vast majority of benchmarks, matching or outperforming the best available models on 24 out of 26 variant effect prediction tasks. In doing so, it addresses longstanding trade-offs in genome modeling by capturing both distal regulatory context and nucleotide-level detail, enabling more accurate predictions of how genetic variants influence gene regulation. This report provides a detailed overview of AlphaGenome’s methodology – including its architecture, training strategy, improvements over the earlier Enformer model, variant effect prediction performance across multiple modalities, and speculations on integrating AlphaGenome with genomic language models."
  },
  {
    "objectID": "blog-posts/alphagenome-evolution-genomic-modeling.html#methodology",
    "href": "blog-posts/alphagenome-evolution-genomic-modeling.html#methodology",
    "title": "AlphaGenome Evolution: Advancing Regulatory Variant Prediction and Genomic Modeling",
    "section": "Methodology",
    "text": "Methodology\n\nModel Architecture: U-Net Design with Transformers and Multi-Resolution Output\nU-Net–style Encoder–Decoder: AlphaGenome’s architecture follows a U-Net inspired design, consisting of an encoder that progressively downsamples the input sequence, a “Transformer Tower” that integrates long-range information, and a decoder that upsamples back to high resolution outputs. The encoder–decoder structure is similar to those in image segmentation (U-Net) but adapted for 1D genomic sequences. The sequence encoder uses multiple convolutional blocks and pooling to reduce the sequence length while increasing channel depth, extracting hierarchical features at increasing scales. Starting from 1 bp resolution with 768 channels, the encoder downsamples through 7 stages (with max-pooling by 2 at each stage) to a final 128 bp resolution latent representation with 1536 channels. Convolutional filters capture local sequence motifs (e.g. transcription factor binding sites, splice signals) needed for base-level precision. Residual connections (“skip” connections) from the encoder layers are carried into the decoder, as in U-Net, to preserve fine-grained spatial information. The sequence decoder upsamples the latent representation back toward higher resolutions, merging with encoder skip features to output predictions at multiple scales (including single-nucleotide resolution for certain tracks). This multi-resolution design allows AlphaGenome to output different genomic tracks at the appropriate resolution for each assay – for example, base-pair resolution for splice site usage or transcription start sites, and binned resolution for broader signals like contacts.\nTransformer Layers for Long-Range Context: Between encoder and decoder, AlphaGenome inserts a stack of Transformer blocks (the “Transformer Tower”) operating on the 128 bp-resolution encoded sequence. Nine transformer layers model coarse but long-range dependencies across the entire 1 Mb input, such as distal enhancer–promoter interactions or coordinated chromatin state changes. These transformers use multi-head self-attention to allow any position in the 1 Mb sequence to attend to any other, capturing genomic interactions spanning hundreds of kilobases. To manage the computational load of such a long sequence, AlphaGenome employs multi-query attention (multiple query heads but shared key/value) to reduce memory, and applies Rotary Positional Embeddings (RoPE) for encoding positional information over 8192 positions (which correspond to 1,048,576 bp at 128 bp resolution). Additionally, attention logits are stabilized with techniques like soft clipping (constraining values to [-5,5]) before softmax. Notably, AlphaGenome introduces a pairwise interaction bias in the attention: every second transformer block is preceded by an update to a 2D pairwise representation (of size 512×512, representing the 1 Mb region at 2048 bp resolution) that captures spatial contacts between sequence segments. This pairwise matrix (128 channels) is analogous to the approach in AlphaFold, and is used both to produce 3D chromatin contact map predictions and as an attention bias added into the Transformer’s self-attention weights. By injecting this learned pairwise contact bias into the attention layers, the model can more easily learn long-range chromatin loops and interactions while maintaining single-base sensitivity. The final output of the transformer tower is thus a context-enriched sequence embedding (still at 128 bp resolution) along with a pairwise interaction matrix representing coarse chromatin contacts.\nSequence Parallelism for 1 Mb Input: Handling a 1,048,576 bp input with such a deep model is computationally challenging. AlphaGenome leverages sequence parallelism across multiple hardware devices to make this feasible. In practice, the 1 Mb sequence is split into 8 chunks (~131 kb each) which are processed in parallel on 8 interconnected TPUv3 cores, with synchronized communication in the transformer layers. This allows true 1 Mb context to be processed with base-pair resolution output, something that would be memory-prohibitive on a single device. The model totals ~450 million parameters distributed across components (≈20% in encoder conv layers, 28% in transformers, 15% in pairwise/contact blocks, 25% in decoder, 12% in output heads). Despite its scale, the sequence-parallel design enables efficient inference: the final distilled model runs a variant effect prediction in under one second on a modern GPU.\nMulti-Scale Outputs and Heads: AlphaGenome produces predictions at multiple output resolutions to suit different assay types. The task-specific output heads are linear layers or small networks that take the decoder’s sequence embeddings and produce the final track values. For most genomic tracks (e.g. epigenomic signals, accessibility, basewise expression coverage), the model outputs a continuous track of predicted signal per base or per small bin, achieved by upsampling the decoder embeddings back to 1 bp resolution and applying a linear transformation. Some outputs are naturally lower-resolution (for instance, histone ChIP-seq might be averaged in bins, or 3D contacts are at 2048 bp bins), and these heads use the appropriate latent scale. Importantly, AlphaGenome includes a novel mechanism for splice junction count prediction, which is not generated by a single-position linear head. Instead, predicting a junction (connecting a donor and acceptor site) requires pairing two distant sequence positions. AlphaGenome addresses this by a separate junction module that computes an interaction between the 1D embeddings of predicted donor and acceptor sites to produce a count for that specific exon-exon junction. In essence, it identifies all candidate donor/acceptor pairs from the decoder’s 1 bp resolution embeddings and assigns each a score, enabling prediction of splice junction read counts (and even novel exon connections) that standard sequence models could not directly output. This is a unique architectural feature of AlphaGenome, allowing it to model splicing outcomes at the level of individual introns (splice junctions) in addition to per-site usage. Overall, by combining convolutional local feature extractors, transformers for global context, and a U-net decoder for high-resolution reconstruction, AlphaGenome’s architecture is able to capture patterns ranging from transcription factor binding motifs to multi-kilobase enhancer looping, all within one model.\n\n\nTraining Strategy: Two-Stage Pre-training and Distillation\nTraining AlphaGenome to robustly predict genome-wide profiles and variant effects required a carefully designed two-stage training process. The authors employed a pre-training stage on experimental data followed by a distillation stage to produce a single efficient model for variant effect prediction.\nStage 1 – Cross-Validated Pre-training on Experimental Data: In the first stage, AlphaGenome was trained directly on the vast compendium of experimental genomics data (profiles of chromatin marks, RNA-seq coverage, etc.) using a form of cross-validation training. The genome was split into four folds, each comprising 25% of the human (and mouse) reference genome segments. Fold-specific models were trained on 3 out of 4 folds (75% of the genome) and validated on the held-out fold. This yields four independent teacher models, each having seen most of the genome but tested on a unique held-out portion. In addition, a separate set of “all-folds” teacher models were trained on all available data (100% of the genome intervals) to maximize use of training data. These all-folds models represent what the model can achieve when not holding out any part of the genome, and effectively serve as an ensemble of experts that have seen the full diversity of sequences. Throughout pre-training, data augmentation was applied: input 1 Mb sequences were randomly shifted or reverse-complemented to augment context and reduce positional biases. The model was trained to minimize error between predicted tracks and actual experimental tracks, producing high-fidelity genome track predictors. By the end of this stage, AlphaGenome had learned to accurately predict functional genomics tracks on sequence segments it had never seen (testing on held-out folds) – establishing its strong generalization for genome-wide prediction. Notably, as a fully multimodal model, it was simultaneously learning splicing patterns, gene expression levels, chromatin signals, and more, across thousands of output channels.\nStage 2 – Distillation into a Single Student Model: While the fold-specific models demonstrated performance, using them for variant effect prediction would require ensembling or making multiple predictions per variant (one for each model). Instead, AlphaGenome’s second stage produces one unified model via knowledge distillation. The all-folds teacher models (from stage 1) are frozen, and a single student model (with the same architecture) is trained to mimic the teachers’ outputs on new sequences. In this stage, the student takes augmented input sequences (including simulated variant perturbations) and must predict the outputs that the ensemble of teachers would have produced. Essentially, the student is learning a smoothed, averaged representation of the multiple teacher models. This approach has two key benefits: (1) The student model ends up more robust and accurate on variant effect prediction than any single direct model. Distillation has been shown to improve robustness and VEP accuracy in prior work, likely because the student learns to generalize the consensus of many teachers, reducing overfitting to idiosyncrasies. (2) The single student is computationally efficient, replacing what would otherwise be an ensemble of 4+ large models. The resulting distilled AlphaGenome can score a variant’s effects on all modalities with one forward pass in under one second on modern hardware. This is crucial for practical use in scanning millions of variants. During distillation, random sequence augmentations and even mutational perturbations were applied to the input, so the student learns to handle variants implicitly. By learning from the teacher ensemble’s predictions (which have effectively seen the entire genome), the student generalizes well even to variants in novel sequences.\nImplications for Variant Effect Prediction: The two-stage strategy means that AlphaGenome’s final model is not directly trained on ground-truth variant effect labels, but rather inherits its variant-scoring ability from the accuracy of the teacher models on reference genome tracks. Because the student sees mutated sequences during distillation and must predict the teachers’ outputs for both reference and altered sequences, it effectively learns to translate sequence changes into output differences. This distilled model proved to be exceptionally strong in variant effect prediction tasks, outperforming direct training in many cases. The authors note that ensembling across several independently trained models can improve variant effect performance, but their distilled single model achieves comparable or better accuracy without ensembling. In summary, the training pipeline first teaches AlphaGenome what patterns to predict (by fitting experimental data), and then teaches it how to efficiently approximate an ensemble of those predictors in one network – yielding a model that is both powerful and practical for scoring variants.\n\n\nComparison to Enformer: Context Length, Resolution, and Directional Prediction\nImproving on Enformer’s Context vs. Resolution Trade-off: One of AlphaGenome’s key achievements is combining long-range genomic context with single-base resolution outputs, improving upon limitations of the Enformer model. Enformer (Avsec et al., 2021) was an earlier transformer-based model that processed ~200 kb of DNA and predicted epigenomic tracks at a fixed 128 bp output bin size. This meant Enformer could capture distal enhancers but only produced low-resolution tracks, blurring fine features like splice sites. AlphaGenome extends the input length to 1 Mb (5× longer) and outputs many tracks at 1 bp resolution, thanks to its U-Net decoder design. By downsampling and then upsampling with skip connections, AlphaGenome preserves nucleotide-level information despite the large receptive field. In contrast to Enformer’s 128 bp discretization, AlphaGenome can pinpoint effects at individual bases (e.g. exact splice donor positions or transcription start sites) while still modeling contacts and enhancer–promoter interactions hundreds of kilobases away. This effectively eliminates the trade-off: AlphaGenome captures long-range interactions without sacrificing resolution. As noted in the paper, previous models like Enformer or its successor Borzoi had to reduce resolution (to 128 bp or even 32 bp bins) to handle &gt;200 kb sequences, missing fine regulatory elements. AlphaGenome’s architecture resolves this by using sequence parallelism and a multi-scale decoder, achieving both breadth and detail in predictions. A direct head-to-head benchmark confirmed this improvement: when retrained to predict Enformer’s own track targets, AlphaGenome attained higher accuracy than Enformer even on Enformer’s task, despite using the full 1 Mb input and base-level features. In other words, AlphaGenome can do what Enformer did, only better – plus much more. Figure 1 of the AlphaGenome paper summarizes that across various genome-wide prediction tasks (covering RNA-seq, chromatin marks, etc.), AlphaGenome had performance gains in the range of +5% to +40% relative to the best prior models, Enformer included. For instance, it improved Pearson correlation for cell-type-specific gene expression by +17.4% over Borzoi (a model that itself builds on Enformer). These results demonstrate that by addressing Enformer’s limitations – extending context and sharpening resolution – AlphaGenome yields more accurate predictions of genomic function.\nAddressing Variant Effect Directionality: Another notable shortcoming of Enformer was its difficulty in predicting the direction of variant effects (i.e. whether a mutation increases or decreases a functional readout). Enformer could predict changes in track intensity, but often struggled to correctly classify the sign of effect, especially for gene expression QTLs. AlphaGenome explicitly tackles this directional prediction problem and shows marked improvements. In evaluations on eQTLs (expression quantitative trait loci), AlphaGenome was able to predict not just the magnitude of expression change but also the sign (direction) of the effect with significantly better accuracy than previous models. For example, compared to Borzoi (the prior state-of-the-art and an improved Enformer-like model), AlphaGenome improved the area under ROC for predicting eQTL effect direction from 0.75 to 0.80 (a substantial +5% increase in classification performance). It also achieved a higher Spearman rho (0.49 vs 0.39) for correlating predicted vs observed effect sizes (magnitudes). These gains indicate that AlphaGenome can more reliably tell if a variant will up-regulate or down-regulate a gene’s expression, which Enformer and others struggled with. The improvement comes from multiple factors: the multimodal outputs (AlphaGenome predicts downstream consequences on many tracks, providing richer clues to infer direction), the distillation training (which may smooth out noise and make the model more confident in sign), and possibly architectural changes like basepair-resolution outputs that capture subtle asymmetric effects. As a concrete example, AlphaGenome’s paper highlights a variant near the TAL1 oncogene where Enformer-based scoring had difficulty, but AlphaGenome clearly predicted that the mutation activated an enhancer to increase TAL1 expression. More generally, AlphaGenome was shown to recover far more true positive eQTLs at high precision when requiring correct direction: at 90% predicted sign accuracy, it captured 2× more eQTLs than the previous model (41% vs 19% of variants). This indicates that researchers can now filter variant hits by predicted direction with much greater confidence. In summary, AlphaGenome largely overcomes Enformer’s directional limitation by providing a model that not only predicts the magnitude of molecular changes caused by a variant but also correctly infers the polarity of the effect (gain or loss of function) across modalities. This is a critical advancement for variant interpretation, as knowing how a variant perturbs a gene or element (increasing vs decreasing activity) is key to linking variants to phenotypic outcomes."
  },
  {
    "objectID": "blog-posts/alphagenome-evolution-genomic-modeling.html#variant-effect-prediction-across-modalities",
    "href": "blog-posts/alphagenome-evolution-genomic-modeling.html#variant-effect-prediction-across-modalities",
    "title": "AlphaGenome Evolution: Advancing Regulatory Variant Prediction and Genomic Modeling",
    "section": "Variant Effect Prediction Across Modalities",
    "text": "Variant Effect Prediction Across Modalities\nAlphaGenome was evaluated on a comprehensive suite of 26 variant effect prediction (VEP) benchmarks spanning diverse molecular phenotypes. The model demonstrated high accuracy in predicting variant consequences across multiple modalities, including splicing, gene expression, chromatin accessibility, and transcription factor (TF) binding. Here we examine each modality in turn, highlighting AlphaGenome’s performance and novel methodological features like splice junction modeling and composite scoring.\n\nSplicing Variants: Unified Splice Site, Usage, and Junction Prediction\nOne of AlphaGenome’s most innovative aspects is its treatment of splicing. Previous models like SpliceAI focused on predicting whether a variant disrupts canonical splice sites (donors/acceptors), and others like Pangolin predicted splice site usage (percent spliced in, PSI) changes, but none directly predicted the formation of new splice junctions. AlphaGenome is the first system to jointly predict all three levels of splicing outcomes: (1) the probability of each nucleotide being a splice donor or acceptor, (2) the usage of each splice site (proportion of transcripts using that site), and (3) the presence and read count of specific splice junctions (introns) connecting two sites. By integrating these, AlphaGenome provides a holistic view of how a variant will alter splicing patterns. In practice, this means AlphaGenome can detect subtle splicing changes such as cryptic splice site activation, exon skipping, or novel exon creation, which are often missed by models that only score nearest splice sites.\nPerformance on Splicing Benchmarks: The model’s comprehensive splicing prediction translates into state-of-the-art results on numerous splicing variant benchmarks. AlphaGenome’s authors constructed a unified splicing variant scorer that combines the model’s various splicing outputs into a single composite score for a variant. This involves computing separate sub-scores for splice site disruption, changes in splice site usage (ΔPSI), and any new or lost junctions, then summing them into a composite metric. When evaluated on fine-mapped sQTLs (splicing QTLs) – variants associated with splicing changes in GTEx – AlphaGenome’s composite scorer achieved the highest accuracy in distinguishing true sQTL variants from negatives. It outperformed prior methods in both “nearby SNP” scenarios (variants within 200 bp of a splice site) and more distant variants affecting splicing up to 10 kb away. Similarly, on a task of predicting rare splice-disrupting variants (variants causing aberrant splicing in GTEx outlier samples), AlphaGenome again led both in unsupervised ranking and in a supervised setting.\nNotably, in ClinVar pathogenicity classification for variants affecting splicing, AlphaGenome’s splicing scores beat the previous best method (Pangolin) in every category. For example, for deep intronic or synonymous variants that sometimes create cryptic splice sites, AlphaGenome achieved an auPRC of 0.66 vs 0.64 by Pangolin. In the “splice region” category (variants near exon-intron junctions), it scored 0.57 auPRC vs 0.55 for Pangolin, and even for missense variants (where splicing changes are an off-target effect) it edged out the competition (0.18 vs 0.16). The only benchmark where AlphaGenome did not rank first was a high-throughput splicing reporter assay (MFASS) for which Pangolin slightly exceeded it (auPRC 0.54 vs 0.51). Even there, AlphaGenome still outperformed other tools like SpliceAI and DeltaSplice (each 0.49). Interestingly, the authors found that the splice junction-specific sub-score alone (ignoring site disruption scores) was extremely powerful: it outperformed all prior methods on 5 of 7 benchmarks by itself. This underscores the value of explicit junction prediction – by modeling the creation or loss of specific exon-exon links, AlphaGenome captures effects that purely site-based models might miss. Overall, AlphaGenome was declared a “state-of-the-art splicing VEP model”, achieving SOTA on 6 of 7 tests. The rich splicing output not only improves accuracy but also provides mechanistic insight. For example, AlphaGenome correctly predicted a known case of exon skipping: a 4 bp deletion in the DLG1 gene that causes an exon to be skipped in arterial tissue. The model’s predictions showed reduced usage of the exon’s splice site, disappearance of junctions that include that exon, appearance of a junction skipping over it, and loss of RNA-seq coverage for that exon – precisely matching the experimental observation. In another example, it captured a novel splice junction created by a variant in the COL6A2 gene (Aorta tissue), which led to an extended exon; AlphaGenome’s junction and coverage predictions mirrored the GTEx RNA-seq evidence of that new splicing event. These case studies highlight how AlphaGenome’s fine-grained splicing predictions can pinpoint the exact nature of splicing alterations caused by variants, an ability that was lacking in earlier general models."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my genomics blog! I’m passionate about exploring the intersection of computational biology, machine learning, and genomics research.\n\n\nThis blog covers several key areas in modern genomics:\n\nComputational Genomics: Analysis and interpretation of genomic data using computational methods\nMachine Learning in Biology: Applications of ML and AI in genomic prediction and modeling\nRegulatory Genomics: Understanding how regulatory elements control gene expression\nEvolutionary Genomics: Studying genomic evolution and its implications\nClinical Genomics: Translating genomic insights into clinical applications\n\n\n\n\nPosts are organized in the blog-posts/ directory, with each post focusing on synthesizing insights from recent research papers. The goal is to provide both technical depth and accessible explanations of complex genomic concepts.\n\n\n\nFor questions or suggestions about the blog content, feel free to reach out through the repository.\n\nThis blog is built using Quarto, a modern scientific and technical publishing system."
  },
  {
    "objectID": "about.html#focus-areas",
    "href": "about.html#focus-areas",
    "title": "About",
    "section": "",
    "text": "This blog covers several key areas in modern genomics:\n\nComputational Genomics: Analysis and interpretation of genomic data using computational methods\nMachine Learning in Biology: Applications of ML and AI in genomic prediction and modeling\nRegulatory Genomics: Understanding how regulatory elements control gene expression\nEvolutionary Genomics: Studying genomic evolution and its implications\nClinical Genomics: Translating genomic insights into clinical applications"
  },
  {
    "objectID": "about.html#blog-structure",
    "href": "about.html#blog-structure",
    "title": "About",
    "section": "",
    "text": "Posts are organized in the blog-posts/ directory, with each post focusing on synthesizing insights from recent research papers. The goal is to provide both technical depth and accessible explanations of complex genomic concepts."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "",
    "text": "For questions or suggestions about the blog content, feel free to reach out through the repository.\n\nThis blog is built using Quarto, a modern scientific and technical publishing system."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Adrian’s Generative Genomics Blog",
    "section": "",
    "text": "This blog explores the intersection of genomics, generative modeling, and bioinformatics. Here you’ll find insights into cutting-edge research, computational methods, and their applications in understanding the genome.\n\n\n\nAlphaGenome Evolution: Advancing Regulatory Variant Prediction and Genomic Modeling - A comprehensive exploration of AlphaGenome’s capabilities in regulatory variant prediction and its integration with advanced DNA language models for enhanced genomic modeling.\n\n\n\n\nThis blog serves as a platform for sharing insights from recent research papers, particularly focusing on computational genomics and language model applications in biology. Each post aims to provide both technical depth and accessible explanations of complex genomic concepts.\n\nBuilt with Quarto"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Adrian’s Generative Genomics Blog",
    "section": "",
    "text": "AlphaGenome Evolution: Advancing Regulatory Variant Prediction and Genomic Modeling - A comprehensive exploration of AlphaGenome’s capabilities in regulatory variant prediction and its integration with advanced DNA language models for enhanced genomic modeling."
  },
  {
    "objectID": "index.html#about-this-blog",
    "href": "index.html#about-this-blog",
    "title": "Adrian’s Generative Genomics Blog",
    "section": "",
    "text": "This blog serves as a platform for sharing insights from recent research papers, particularly focusing on computational genomics and language model applications in biology. Each post aims to provide both technical depth and accessible explanations of complex genomic concepts.\n\nBuilt with Quarto"
  },
  {
    "objectID": "blog-posts/index.html",
    "href": "blog-posts/index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Welcome to my collection of blog posts on genomics, generative modeling, and bioinformatics research.\n\n\n\n\nAugust 2, 2025\nA comprehensive exploration of AlphaGenome’s capabilities in regulatory variant prediction and its integration with advanced DNA language models for enhanced genomic modeling. This post synthesizes insights from two key papers on AlphaGenome’s evolution and its applications in genomic prediction.\nTopics covered: - Regulatory variant effect prediction - Integration with advanced DNA language models for enhanced genomic modeling - Multi-scale genomic modeling approaches - Clinical and research applications - Future directions in genomic prediction\n\nMore posts coming soon!"
  },
  {
    "objectID": "blog-posts/index.html#recent-posts",
    "href": "blog-posts/index.html#recent-posts",
    "title": "Blog Posts",
    "section": "",
    "text": "August 2, 2025\nA comprehensive exploration of AlphaGenome’s capabilities in regulatory variant prediction and its integration with advanced DNA language models for enhanced genomic modeling. This post synthesizes insights from two key papers on AlphaGenome’s evolution and its applications in genomic prediction.\nTopics covered: - Regulatory variant effect prediction - Integration with advanced DNA language models for enhanced genomic modeling - Multi-scale genomic modeling approaches - Clinical and research applications - Future directions in genomic prediction\n\nMore posts coming soon!"
  }
]